{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from sklearn.externals.six import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = df_train.select_dtypes('object').columns\n",
    "for col in dummy_cols:\n",
    "    categories = df_train[col].unique()\n",
    "\n",
    "    df_train[col] = pd.Categorical(df_train[col], categories=categories)\n",
    "    df_test[col] = pd.Categorical(df_test[col], categories=categories)\n",
    "\n",
    "df_train = pd.get_dummies(df_train)\n",
    "df_test = pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                   int64\n",
      "bmi                 float64\n",
      "children              int64\n",
      "sex_male              uint8\n",
      "sex_female            uint8\n",
      "smoker_no             uint8\n",
      "smoker_yes            uint8\n",
      "region_southeast      uint8\n",
      "region_southwest      uint8\n",
      "region_northwest      uint8\n",
      "region_northeast      uint8\n",
      "dtype: object\n",
      "     0\n",
      "0    0\n",
      "1    2\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "..  ..\n",
      "395  0\n",
      "396  0\n",
      "397  2\n",
      "398  0\n",
      "399  0\n",
      "\n",
      "[400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_test.dtypes)\n",
    "print(pd.DataFrame(y_test_f, index=list(x_test_f.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       32.665465\n",
       "1       29.798725\n",
       "2       32.722029\n",
       "3       38.429831\n",
       "4       29.641854\n",
       "          ...    \n",
       "1595    32.772830\n",
       "1596    37.189564\n",
       "1597    43.281979\n",
       "1598    28.232308\n",
       "1599    28.997113\n",
       "Name: bmi, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n",
      "400\n",
      "      0\n",
      "13    0\n",
      "23    2\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "...  ..\n",
      "1968  0\n",
      "1972  0\n",
      "1979  2\n",
      "1983  0\n",
      "1995  0\n",
      "\n",
      "[400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "x = df_train.drop([\"id\",\"charges\"],axis=1)\n",
    "y = df_train[\"charges\"]\n",
    "x_test_f = df_test.drop(\"id\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,shuffle=True, random_state=31, stratify=y)\n",
    "lr_tree = DecisionTreeClassifier(max_depth=4)\n",
    "lr_tree.fit(x_train,y_train)\n",
    "print(lr_tree.score(x_test, y_test))\n",
    "\n",
    "dot_data = export_graphviz(lr_tree)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('test', format='png')\n",
    "\n",
    "y_test_f = lr_tree.predict(x_test_f)\n",
    "print(len(y_test_f))\n",
    "\n",
    "df_submit = pd.DataFrame(y_test_f, index=list(df_test[\"id\"]))\n",
    "df_submit.to_csv(\"submit.csv\", header = False)\n",
    "print(df_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satoshi/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 2 0\n",
      " 0 2 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 0 0 0 1 1 0\n",
      " 2 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 2 0 0 0 0 0 0 0 2 0 1 1\n",
      " 0 0 0 0 0 2 0 1 0 0 0 2 0 2 0 1 0 0 1 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 2 0 0 0 0 0 0 0 0 2 2 0 1 0 0 2 0 0 0 2 0 0 0 1 0 0 1 0 0 0 0 0 0 2 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 2 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 2 0 0 0 2 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "class data_analysis:\n",
    "    def __init__(self, df_train_name, df_test_name, header_ck = True):\n",
    "        self.df_train = pd.read_csv(df_train_name, header = header_ck)\n",
    "        self.df_test = pd.read_csv(df_test_name, header = header_ck)\n",
    "        \n",
    "    # 使用する変数と目的変数を指定して、抽出および説明変数と目的変数で分離\n",
    "    def extract_df(self, extract_list, object_value):\n",
    "        self.df_train_x = self.df_train.drop(extract_list, axis=1)\n",
    "        self.df_train_y = self.df_train(object_value)\n",
    "        self.df_test_x = self.df_test.drop(extract_list, axis=1)\n",
    "        \n",
    "    # validation\n",
    "    def validation_product(self, size = 0.75, stratify = True):\n",
    "        if stratify == True:\n",
    "            self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(self.df_train_x, self.df_train_y, train_size=size, shuffle=True, random_state=31, stratify=self.df_train_y)\n",
    "        else:\n",
    "            self.x_train, self.x_valid, self.y_train, self.y_valid = train_test_split(self.df_train_x, self.df_train_y, train_size=size, shuffle=True, random_state=31)\n",
    "\n",
    "    # XGBoostにおけるグリッドサーチによるパラメータ推定, task=[\"classify\", \"regression\"]\n",
    "    def xgb_estimate_params(self, task = \"classify\", binary_task = True):\n",
    "        if task == \"classify\":\n",
    "            params = {\n",
    "                \"learning_rate\":np.arange(0.1,0.6,0.1),\n",
    "                \"max_depth\":np.arange(5,8,1),\n",
    "                \"subsample\":np.arange(0.6,1.1,0.1),\n",
    "                \"colsample_bytree\":np.arange(0.6,1.1,0.1),\n",
    "                \"reg_lambda\":np.arange(0.6,1.1,0.1)\n",
    "            }\n",
    "            if binary_task:\n",
    "                clf_xgb = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "            else:\n",
    "                clf_xgb = xgb.XGBClassifier(objective=\"multi:softmax\")\n",
    "            \n",
    "            kF = KFold(n_splits = 5, shuffle=True, random_state=31)\n",
    "            self.gs = GridSearchCV(clf_xgb, params, cv = kF, n_jobs=-1, scoring = \"accuracy\")\n",
    "            self.gs.fit(self.df_train_x,self.df_train_y)\n",
    "            print(\"{}, {}\".format(self.gs.best_estimator_, self.gs.best_score_))\n",
    "    \n",
    "    # XGBoostによるテスト\n",
    "    def xgb_test(self, signate = False, col_list):\n",
    "        self.df_test_y = self.gs.predict(self.df_test_x)\n",
    "        if signate = True:\n",
    "            submit2signate(self.df_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.30000000000000004, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=0.7,\n",
      "              tree_method='exact', use_label_encoder=True,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "0.8608333333333332\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_estimator_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_estimate_params(df_train):\n",
    "    x = df_train.drop([\"id\",\"charges\"],axis=1)\n",
    "    y = df_train[\"charges\"]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,shuffle=True, random_state=31, stratify=y)\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\":[0.3]\n",
    "        \"max_depth\":[5],\n",
    "        \"subsample\":[0.7]\n",
    "    #     \"learning_rate\":np.arange(0.1,0.6,0.1),\n",
    "    #     \"max_depth\":np.arange(5,8,1),\n",
    "    #     \"subsample\":np.arange(0.6,1.1,0.1),\n",
    "    #     \"colsample_bytree\":np.arange(0.6,1.1,0.1),\n",
    "    #     \"reg_lambda\":np.arange(0.6,1.1,0.1)\n",
    "    }\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,shuffle=True, random_state=31, stratify=y)\n",
    "\n",
    "    clf_xgb = xgb.XGBClassifier(objective=\"multi:softmax\")\n",
    "    kF = KFold(n_splits = 5, shuffle=True, random_state=31)\n",
    "    gs = GridSearchCV(clf_xgb, params, cv = kF, n_jobs=-1, scoring = \"accuracy\")\n",
    "    kF = KFold(n_splits = 5, shuffle=True, random_state=31)\n",
    "    # score = cross_validate(clf_xgb, x, y, cv=kF, scoring=\"accuracy\")\n",
    "    gs.fit(x_train,y_train)\n",
    "    print(gs.predict(x_test))\n",
    "\n",
    "def xgb_test(df_test):\n",
    "    x_test_f = df_test.drop(\"id\", axis=1)\n",
    "    params = {\n",
    "        \"learning_rate\":[0.3]\n",
    "        \"max_depth\":[5],\n",
    "        \"subsample\":[0.7]\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
